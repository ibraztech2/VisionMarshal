{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibraztech2/VisionMarshal/blob/main/vision_marshal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqaagEV1-tG5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5TMspYIHY3F"
      },
      "source": [
        "#  ***Backend Setup***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yu_BxP0rqL--"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/smart_traffic_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vooaEa-rSl4"
      },
      "source": [
        "## `1. Drive Mount`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zbK7jJIk-y3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bed00b-5a3a-4df1-8eec-aa2995771aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "# Check the version of cuda compiler available\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C9uqNh2lL89Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e8ade-2311-49d8-8ab5-0f0322dea6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#Mount drive to get acces to video input\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqR0Y-0drcLB"
      },
      "source": [
        "## `2. MMAction2 installation And Dependencies Resolution`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzXxVY5ak_Ke"
      },
      "source": [
        "### 1. Downgrade Pytorch, Numpy, Transformers, Torchvision, Torchaudio to compatible Version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B05-Bl_b9mOy",
        "outputId": "71117811-cc69-4379-9393-6557238a6fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: transformers 4.53.2\n",
            "Uninstalling transformers-4.53.2:\n",
            "  Successfully uninstalled transformers-4.53.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m657.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from torchvision==0.16.0+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu118) (1.3.0)\n",
            "Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, numpy, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# !pip uninstall -y torch torchvision torchaudio mmcv mmcv-full mmengine numpy torchao transformers\n",
        "!pip uninstall -y torch torchvision torchaudio numpy transformers\n",
        "\n",
        "\n",
        "# Install PyTorch 2.1.0 with CUDA 11.8\n",
        "!pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5OPOlpVshkG"
      },
      "source": [
        "### 2. Install MMCV and MMengine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGWv3BSN_OVN"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Install MMCV 2.1.0 and mmengine\n",
        "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1/index.html\n",
        "!pip install mmengine==0.10.3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ZJwIFIs9yW"
      },
      "source": [
        "### 3. Install OpenMIM ( command line tool for MMAction and  MMdetection )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wol3TNb__iO8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Install OpenMIM\n",
        "!pip install -U openmim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InmJuwGAwGG8"
      },
      "source": [
        "### 4. Build MMaction   from source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V9i7z6gE_yTO"
      },
      "outputs": [],
      "source": [
        "# Clone MMAction from Github\n",
        "\n",
        "!git clone https://github.com/open-mmlab/mmaction2.git\n",
        "%cd mmaction2\n",
        "!pip install -e .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IKyEHT5wgdf"
      },
      "source": [
        "### 5. Install Numpy compatible Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut7_Epqa_24m"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzgQqkp-wuj5"
      },
      "source": [
        "### 6. Download  Temporal Segment Network ( Built on Imagenet ) configuration file and  checkpoint file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3vHJPxlL_-xN"
      },
      "outputs": [],
      "source": [
        "# downloading the configuration file to the given location\n",
        "\n",
        "!mim download mmaction2 --config tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb --dest /content/mmaction2/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYhHxFoQxqfj"
      },
      "source": [
        "### 7. Testing Setup for :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvqoX28CHY3L"
      },
      "source": [
        "#### 1.  MMAction Setup on a demo video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hed77KcmABqv"
      },
      "outputs": [],
      "source": [
        "# The demo.mp4 and label_map_k400.txt are both from Kinetics-400\n",
        "!python /content/mmaction2/demo/demo.py /content/mmaction2/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py \\\n",
        "    /content/mmaction2/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb_20220906-2692d16c.pth \\\n",
        "    /content/mmaction2/demo/demo.mp4 /content/mmaction2/tools/data/kinetics/label_map_k400.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5mGyQQ_HY3M"
      },
      "source": [
        "#### 2. MMDetcttion on a Demo picture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtrhaE-i_c1E"
      },
      "outputs": [],
      "source": [
        "!python /content/mmdetection/demo/image_demo.py /content/mmdetection/demo/demo.jpg /content/mmdetection/rtmdet_tiny_8xb32-300e_coco.py --weights /content/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --device cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSs6O-jsdT8S"
      },
      "source": [
        "## `3.  App File, Requirement and Checkpoint setup `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjxwwFHdHY3N"
      },
      "source": [
        "### 1. Download Checkpoints  Files for Action recognition [Timesformer](https://mmaction2.readthedocs.io/en/latest/model_zoo/recognition.html#timesformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L193NiuPRMGR"
      },
      "outputs": [],
      "source": [
        "# Create Project  Tree Struscture\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/content/smart_traffic_detector\", exist_ok=True)\n",
        "\n",
        "os.makedirs(\"/content/smart_traffic_detector/debug\", exist_ok=True)\n",
        "os.makedirs(\"/content/smart_traffic_detector/main\", exist_ok=True)\n",
        "os.makedirs(\"/content/smart_traffic_detector/checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"/content/smart_traffic_detector/clips\", exist_ok=True)\n",
        "os.makedirs(\"/content/smart_traffic_detector/violations\", exist_ok=True)\n",
        "os.makedirs(\"/content/smart_traffic_detector/alerts\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5NxX7Z9_dUuF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download Model  Checkpoint  for ACtion Recognition\n",
        "\n",
        "!wget https://download.openmmlab.com/mmaction/v1.0/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb_20220815-78f05367.pth -P /content/smart_traffic_detector/checkpoints\n",
        "#!wget https://download.openmmlab.com/mmaction/v1.0/recognition/slowfast/slowfast_r101_8xb8-8x8x1-256e_kinetics400-rgb/slowfast_r101_8xb8-8x8x1-256e_kinetics400-rgb_20220818-9c0e09bd.pth -P /content/smart_traffic_detector/checkpoints\n",
        "#!wget https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth -P /content/smart_traffic_detector/checkpoints\n",
        "!wget https://github.com/open-mmlab/mmaction2/blob/main/tools/data/kinetics/label_map_k400.txt -P /content/smart_traffic_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFRsm2CJHY3O"
      },
      "source": [
        "### 3. Write App file and  Requirements file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7jptMM1HY3O"
      },
      "source": [
        "#### 1. write Requirements File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOLWSZx7RzPZ"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "\n",
        "# Core video processing\n",
        "moviepy==1.0.3\n",
        "opencv-python==4.9.0.80\n",
        "ffmpeg-python==0.2.0\n",
        "\n",
        "# Object detection (YOLOv8)\n",
        "ultralytics==8.1.5\n",
        "\n",
        "\n",
        "# Email alerts\n",
        "certifi\n",
        "email-validator\n",
        "\n",
        "# Streamlit (optional for dashboard)\n",
        "streamlit==1.27.2\n",
        "\n",
        "# For parsing configs and data\n",
        "PyYAML==6.0\n",
        "numpy==1.24.4\n",
        "pandas\n",
        "\n",
        "# Youtube  video library\n",
        "yt-dlp\n",
        "pyngrok\n",
        "\n",
        "\n",
        "\n",
        "# frontend and tunnel library\n",
        "pyngrok\n",
        "streamlit\n",
        "ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTC6HxO8sNwW"
      },
      "source": [
        "#### 2. write App file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIysQCDzolR8"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/smart_traffic_detector/main/main_app.py\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "from typing import Dict, List, Any, Set\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "from mmaction.apis import init_recognizer, inference_recognizer\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------\n",
        "# Environment Project Tree Structure Setup\n",
        "#--------------------------------------------\n",
        "\n",
        "work_dir = \"/content/smart_traffic_detector\"\n",
        "model_config = \"/content/mmaction2/configs/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb.py\"\n",
        "model_checkpoints = \"/content/smart_traffic_detector/checkpoints/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb_20220815-78f05367.pth\"\n",
        "label_map_path = \"/content/smart_traffic_detector/label_map_k400.txt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Video Preprocessing\n",
        "# ------------------------------------------------------------------\n",
        "def split_video_into_clips(video_path: str, output_dir: str = \"clips\", clip_length: int = 10):\n",
        "    \"\"\"Split a  video into tubelets  (smaller clips (seconds))  via ffmpeg.\"\"\"\n",
        "\n",
        "\n",
        "    # Probe duration\n",
        "    result = subprocess.run(\n",
        "        [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "         \"-of\", \"default=noprint_wrappers=1:nokey=1\", video_path],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    duration = float(result.stdout.strip()) if result.stdout.strip() else 0.0\n",
        "\n",
        "    clips = []  # temorarily store splitted clips\n",
        "    for start in range(0, int(duration), clip_length):  # iterate over the clip length at a step size of 10sec\n",
        "        clip_name = f\"{output_dir}/clip_{start:04d}.mp4\"\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\", \"-y\",\n",
        "            \"-ss\", str(start),\n",
        "            \"-i\", video_path,\n",
        "            \"-t\", str(clip_length),\n",
        "            \"-vf\", \"scale=640:384\",\n",
        "            \"-c:v\", \"libx264\",\n",
        "            \"-pix_fmt\", \"yuv420p\",\n",
        "            clip_name\n",
        "        ], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "        clips.append((clip_name, start))\n",
        "    return clips\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Traffic Light Color Detection\n",
        "# ------------------------------\n",
        "\n",
        "def detect_traffic_light_color_simple(image, bbox, debug_id=None):\n",
        "    \"\"\"\n",
        "\n",
        "    Returns: 'red', 'yellow', 'green', or 'unknown'.\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "\n",
        "    # Clip to image bounds\n",
        "    h_img, w_img = image.shape[:2]\n",
        "    x1 = max(0, min(x1, w_img - 1))\n",
        "    x2 = max(0, min(x2, w_img))\n",
        "    y1 = max(0, min(y1, h_img - 1))\n",
        "    y2 = max(0, min(y2, h_img))\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        return \"unknown\"\n",
        "\n",
        "    roi = image[y1:y2, x1:x2]  # Grab the region of interest from the Detected Traffic Light\n",
        "    if roi.size == 0:\n",
        "        return \"unknown\"\n",
        "\n",
        "    # Noise reduction\n",
        "    roi = cv2.GaussianBlur(roi, (3, 3), 0)\n",
        "\n",
        "    # increase contrast\n",
        "    lab = cv2.cvtColor(roi, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "    limg = cv2.merge((cl, a, b))\n",
        "    roi = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # HSV  (Hue Saturation Value)\n",
        "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Split into 3 horizontal sections: Red Yellow and Green which falls in top, Mid and bottom of the traffic light respectively\n",
        "\n",
        "    h = roi.shape[0]\n",
        "    third = max(1, h // 3)\n",
        "    sections = {                     # Grab  each section and store them based on the division\n",
        "        \"red\": hsv[0:third, :],\n",
        "        \"yellow\": hsv[third:2 * third, :],\n",
        "        \"green\": hsv[2 * third:h, :]\n",
        "    }\n",
        "\n",
        "    # Broad HSV ranges (tune per camera)\n",
        "    red_lower1 = np.array([0, 50, 20])\n",
        "    red_upper1 = np.array([10, 255, 255])\n",
        "    red_lower2 = np.array([160, 50, 20])\n",
        "    red_upper2 = np.array([180, 255, 255])\n",
        "\n",
        "    yellow_lower = np.array([15, 40, 40])\n",
        "    yellow_upper = np.array([40, 255, 255])\n",
        "\n",
        "    green_lower = np.array([30, 40, 40])\n",
        "    green_upper = np.array([100, 255, 255])\n",
        "\n",
        "     # Debbug traffic light section to detetct the correct light\n",
        "    scores = {}\n",
        "    if debug_id:\n",
        "        dbg_dir = os.path.join(work_dir, \"debug\")\n",
        "        os.makedirs(dbg_dir, exist_ok=True)\n",
        "        cv2.imwrite(os.path.join(dbg_dir, f\"tl_{debug_id}.jpg\"), roi)\n",
        "    else:\n",
        "        dbg_dir = None\n",
        "\n",
        "\n",
        "    # masking ROI to get values the approximate shape and the location of the color detected\n",
        "    for color, section_hsv in sections.items():\n",
        "        if color == \"red\":\n",
        "            mask1 = cv2.inRange(section_hsv, red_lower1, red_upper1)\n",
        "            mask2 = cv2.inRange(section_hsv, red_lower2, red_upper2)\n",
        "            mask = cv2.bitwise_or(mask1, mask2)\n",
        "        elif color == \"yellow\":\n",
        "            mask = cv2.inRange(section_hsv, yellow_lower, yellow_upper)\n",
        "        else:  # green\n",
        "            mask = cv2.inRange(section_hsv, green_lower, green_upper)\n",
        "\n",
        "        scores[color] = cv2.countNonZero(mask)  # count the number of the dominant color of each pixel in each section of the traffic light\n",
        "\n",
        "        if dbg_dir:\n",
        "            cv2.imwrite(os.path.join(dbg_dir, f\"{color}_mask_{debug_id}.jpg\"), mask)   # store Detected color\n",
        "\n",
        "    dominant_color = max(scores, key=scores.get)\n",
        "    total_pixels = roi.shape[0] * roi.shape[1]\n",
        "    return dominant_color if total_pixels > 0 and (scores[dominant_color] / float(total_pixels)) > 0.002 else \"unknown\"\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Object Detection using Yolov8\n",
        "# ------------------------------\n",
        "_object_model = YOLO(\"yolov8n.pt\")  # pretrained COCO\n",
        "\n",
        " # Return Respective classs id for the object detcted\n",
        "if isinstance(_object_model.names, dict):\n",
        "    CLASS_NAMES: Dict[int, str] = _object_model.names\n",
        "elif isinstance(_object_model.names, (list, tuple)):\n",
        "    CLASS_NAMES = {i: n for i, n in enumerate(_object_model.names)}\n",
        "else:\n",
        "    CLASS_NAMES = {}\n",
        "\n",
        "\n",
        "def _get_cls_id(box) -> int:\n",
        "    cid = getattr(box, \"cls\", None)\n",
        "    if cid is None:\n",
        "        return -1\n",
        "    try:\n",
        "        return int(cid.item())\n",
        "    except Exception:\n",
        "        try:\n",
        "            return int(cid[0])\n",
        "        except Exception:\n",
        "            return int(cid)\n",
        "\n",
        "\n",
        "def run_object_detection(clip_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Run YOLO object detection on a video file and return list of detections.\"\"\"\n",
        "    results = _object_model(clip_path, stream=True)\n",
        "    detections = []\n",
        "\n",
        "    for frame_id, res in enumerate(results):\n",
        "        frame = res.orig_img  # BGR numpy array\n",
        "\n",
        "        boxes = getattr(res, \"boxes\", None)\n",
        "        if boxes is None or len(boxes) == 0:\n",
        "            continue\n",
        "\n",
        "        for box_idx, box in enumerate(boxes):\n",
        "            cls_id = _get_cls_id(box)\n",
        "            cls_name = CLASS_NAMES.get(cls_id, f\"class_{cls_id}\")\n",
        "\n",
        "            # get bounding box cordinate\n",
        "            xyxy = box.xyxy\n",
        "            if hasattr(xyxy, \"cpu\"):\n",
        "                xyxy = xyxy.cpu().numpy()\n",
        "            xyxy = np.array(xyxy).reshape(-1, 4)[0].tolist()\n",
        "\n",
        "            # confidence check\n",
        "            conf_attr = getattr(box, \"conf\", None)\n",
        "            if conf_attr is not None:\n",
        "                try:\n",
        "                    conf_val = float(conf_attr.item())\n",
        "                except Exception:\n",
        "                    conf_val = float(conf_attr)\n",
        "            else:\n",
        "                conf_val = 0.0\n",
        "\n",
        "            det = {\n",
        "                \"frame\": frame_id,\n",
        "                \"class\": cls_name,\n",
        "                \"confidence\": conf_val,\n",
        "                \"bbox\": xyxy,\n",
        "            }\n",
        "\n",
        "            # Traffic light color classification\n",
        "            if cls_name == \"traffic light\":\n",
        "                debug_id = f\"{os.path.basename(clip_path)}_f{frame_id}_b{box_idx}\"\n",
        "                det[\"color\"] = detect_traffic_light_color_simple(frame, xyxy, debug_id=debug_id)\n",
        "\n",
        "            detections.append(det)\n",
        "\n",
        "    return detections\n",
        "\n",
        "#--------------\n",
        "# Model Loader\n",
        "#-------------\n",
        "def load_model():\n",
        "  _object_model = YOLO(\"yolov8n.pt\")\n",
        "  _action_model = init_recognizer(model_config, model_checkpoints, device=\"cpu\")\n",
        "  return _object_model, _action_model\n",
        "\n",
        "_object_model, _action_model = load_model()\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Action Recognition  Using TimeSformer from MMAction\n",
        "# ----------------------------------------------------\n",
        "\n",
        "\n",
        "with open(label_map_path) as f:\n",
        "    LABEL_MAP = [line.strip() for line in f.readlines()]\n",
        "\n",
        "\n",
        "def run_action_recognition(clip_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Return top-5 action predictions from MMAction2 model.\"\"\"\n",
        "    results = inference_recognizer(_action_model, clip_path)\n",
        "    top5_scores, top5_labels = results.pred_score.topk(5)\n",
        "    return [\n",
        "        {\"label\": LABEL_MAP[int(label)], \"score\": float(score)}\n",
        "        for label, score in zip(top5_labels, top5_scores)\n",
        "    ]\n",
        "\n",
        "# ----------------------\n",
        "# clip Analysis\n",
        "# ----------------------\n",
        "def analyze_clip(clip_path: str, start_time: float):\n",
        "    obj_dets = run_object_detection(clip_path)\n",
        "    act_dets = run_action_recognition(clip_path)\n",
        "    return {\n",
        "        \"clip\": os.path.basename(clip_path),\n",
        "        \"start_time\": start_time,\n",
        "        \"object_detections\": obj_dets,\n",
        "        \"action_detections\": act_dets\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Violation Rules for both Action and Object\n",
        "# -------------------------------------------\n",
        "violation_rules = {\n",
        "    \"driving car\": {\n",
        "        \"required_objects\": [\"traffic light\", \"car\"],\n",
        "        \"threshold\": 0.05,\n",
        "        \"description\": \"🚨 RED LIGHT VIOLATION Detected. \"\n",
        "    },\n",
        "    \"riding a bike\": {\n",
        "        \"required_objects\": [\"bicycle\", \"person\"],\n",
        "        \"threshold\": 0.25,\n",
        "        \"description\": \"Bike riding – check for helmet use\"\n",
        "    },\n",
        "    \"motorcycling\": {\n",
        "        \"required_objects\": [\"motorcycle\", \"person\"],\n",
        "        \"threshold\": 0.25,\n",
        "        \"description\": \"Motorcycle detected – possible no helmet\"\n",
        "    },\n",
        "    \"running\": {\n",
        "        \"required_objects\": [\"person\", \"car\"],  #\n",
        "        \"threshold\": 0.30,\n",
        "        \"description\": \"Jaywalking or unsafe crossing\"\n",
        "    },\n",
        "    \"sitting\": {\n",
        "        \"required_objects\": [\"person\", \"bus stop\"],\n",
        "        \"threshold\": 0.15,\n",
        "        \"description\": \"Loitering at bus stop\"\n",
        "    },\n",
        "    \"standing\": {\n",
        "        \"required_objects\": [\"person\"],\n",
        "        \"threshold\": 0.20,\n",
        "        \"description\": \"Pedestrian blocking road\"\n",
        "    },\n",
        "}\n",
        "\n",
        "# Map logical -> detector labels (yolov8 was trained on COCO with 80 objects)\n",
        "OBJECT_SYNONYMS = {\n",
        "    \"bus stop\": [\"bench\", \"stop sign\"],\n",
        "}\n",
        "\n",
        "\n",
        "def _expand_required_objects(required: Set[str], detected_objects: Set[str]) -> Set[str]:\n",
        "    \"\"\"Expand logical rule objects into actual detector class names where needed.\"\"\"\n",
        "    expanded = set()\n",
        "    for obj_name in required:\n",
        "        if obj_name in detected_objects:\n",
        "            expanded.add(obj_name)\n",
        "            continue\n",
        "        for syn in OBJECT_SYNONYMS.get(obj_name, []):\n",
        "            if syn in detected_objects:\n",
        "                expanded.add(syn)\n",
        "                break\n",
        "        else:\n",
        "            expanded.add(obj_name)  # keep; may remain missing\n",
        "    return expanded\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Violation Detection Logic\n",
        "# -------------------------\n",
        "def detect_violations(result: Dict[str, Any]):\n",
        "    violations = []\n",
        "    obj_dets = result[\"object_detections\"]\n",
        "    detected_objects = set(obj[\"class\"] for obj in obj_dets)\n",
        "\n",
        "    # Aggregate traffic-light colors (any RED beats others)\n",
        "    tl_colors = [obj.get(\"color\", \"unknown\") for obj in obj_dets if obj[\"class\"] == \"traffic light\"]\n",
        "    traffic_light_status = \"unknown\"\n",
        "\n",
        "    # Priority List of colors\n",
        "    if any(c == \"red\" for c in tl_colors):\n",
        "        traffic_light_status = \"red\"\n",
        "    elif any(c == \"yellow\" for c in tl_colors):\n",
        "        traffic_light_status = \"yellow\"\n",
        "    elif any(c == \"green\" for c in tl_colors):\n",
        "        traffic_light_status = \"green\"\n",
        "\n",
        "    for action in result[\"action_detections\"]:\n",
        "        label, score = action[\"label\"], action[\"score\"]\n",
        "        if label not in violation_rules:\n",
        "            continue\n",
        "        rule = violation_rules[label]\n",
        "\n",
        "        if score < rule[\"threshold\"]:\n",
        "            print(f\"⚠️ Action '{label}' below threshold: {score:.2f} < {rule['threshold']}\")\n",
        "            continue\n",
        "\n",
        "        logical_required = set(rule[\"required_objects\"])\n",
        "        required = _expand_required_objects(logical_required, detected_objects)\n",
        "        missing = {r for r in required if r not in detected_objects}\n",
        "\n",
        "\n",
        "\n",
        "        #  Rule-based violation Description\n",
        "        if not missing and traffic_light_status == \"red\":\n",
        "            violations.append({\n",
        "                    \"clip\": result[\"clip\"],\n",
        "                    \"start_time\": result[\"start_time\"],\n",
        "                    \"violation_type\": \"Red Light Violation\",\n",
        "                    \"description\": \"Vehicle crossed / moved during RED light.\",\n",
        "                    \"score\": score,\n",
        "                    \"objects\": list(logical_required),\n",
        "                    \"light\": traffic_light_status\n",
        "                })\n",
        "            print(f\"⚠️ Violation Detected: {label}.\")\n",
        "        else:\n",
        "            print(f\"❌ Missing required objects for {label}: {missing}\")\n",
        "\n",
        "    return violations\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Run Violation Engine on All Clip Results\n",
        "# ----------------------------------------\n",
        "def run_violation_engine(processed_results: List[Dict[str, Any]]):\n",
        "    all_violations = []\n",
        "    for result in processed_results:\n",
        "        v = detect_violations(result)\n",
        "        if v:\n",
        "            all_violations.extend(v)\n",
        "    return all_violations\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# Extract Violation Evidence Clip\n",
        "# --------------------------------\n",
        "def extract_violation_clip(input_path: str, output_path: str, start: float = 0, clip_duration: int = 5):\n",
        "    video = VideoFileClip(input_path)\n",
        "    video_duration = video.duration\n",
        "\n",
        "    # Clamp start\n",
        "    if start >= video_duration:\n",
        "        start = max(0, video_duration - clip_duration)\n",
        "\n",
        "    end = min(start + clip_duration, video_duration)\n",
        "    print(f\"Extracting from {start:.2f}s to {end:.2f}s (clip is {video_duration:.2f}s long)\")\n",
        "\n",
        "    clip = video.subclip(start, end)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "    clip.close()\n",
        "    video.close()\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# Email Alert Stub\n",
        "# ------------------\n",
        "def send_violation_email():\n",
        "    # Implement Sending Email Algorithm that report violators to authority and may also send automated alert with fine to the violator but with license plate reognition\n",
        "    pass\n",
        "\n",
        "\n",
        "# ---------------\n",
        "# Main Funtion\n",
        "# ---------------\n",
        "def main_func(video_path: str):\n",
        "\n",
        "    #  Preprocess video into tubelet\n",
        "    clips = split_video_into_clips(video_path, output_dir=os.path.join(work_dir,\"clips\"))\n",
        "    processed_results = []\n",
        "\n",
        "    # Detect objects + actions per tubelet\n",
        "    for clip_path, start in clips:\n",
        "        print(f\"Analyzing {clip_path} (start at {start}s)...\")\n",
        "        result = analyze_clip(clip_path, start)\n",
        "        processed_results.append(result)\n",
        "\n",
        "    # Save analysed reult\n",
        "    analysed_path = os.path.join(work_dir, \"analysed_results.json\")\n",
        "    with open(analysed_path, \"w\") as f:\n",
        "        json.dump(processed_results, f, indent=2)\n",
        "    print(f\"Analysis saved: {analysed_path}\")\n",
        "\n",
        "    # Check  for vioaltions\n",
        "    violations = run_violation_engine(processed_results)\n",
        "\n",
        "    # Extract evidence clips\n",
        "    for violation in violations:\n",
        "        clip = violation[\"clip\"]\n",
        "        start_time = violation[\"start_time\"]\n",
        "        violation_type = violation[\"violation_type\"]\n",
        "        violation_clip_name = os.path.join(work_dir, f\"clips/{violation_type.replace(' ', '_').lower()}_{clip}\")\n",
        "        print(f\"Saving violation clip as: {violation_clip_name}\")\n",
        "        input_clip_path = f\"/content/smart_traffic_detector/clips/{clip}\"\n",
        "        extract_violation_clip(input_clip_path, violation_clip_name, start_time, clip_duration=5)\n",
        "\n",
        "    # Save violation log\n",
        "    violation_log_path = os.path.join(work_dir,\"violations\", \"violation_log.json\")\n",
        "    with open(violation_log_path, \"w\") as f:\n",
        "        json.dump(violations, f, indent=2)\n",
        "    print(f\"Violation log saved: {violation_log_path}\")\n",
        "\n",
        "    print(\"Pipeline complete.\")\n",
        "    return processed_results, violations\n",
        "\n",
        "\n",
        "# ----------\n",
        "# CLI Entry\n",
        "# ----------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description=\"Smart Traffic Violation Detection Pipeline\")\n",
        "    parser.add_argument(\"--video\", type=str, required=True, help=\"Path to the input video file\")\n",
        "    args = parser.parse_args()\n",
        "    main_func(args.video)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeXxDdepCpPC"
      },
      "source": [
        "#### 4. Write Streamlit App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DshcYxuKoAqx"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/smart_traffic_detector/streamlit_app.py\n",
        "\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import os\n",
        "import json\n",
        "import streamlit as st\n",
        "from main.main_app import main_func\n",
        "\n",
        "# -------------------------------\n",
        "# Streamlit Page Configuration\n",
        "# -------------------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"🚦 Smart Traffic Violation Detector\",\n",
        "    page_icon=\"🚨\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Sidebar Info\n",
        "# -------------------------------\n",
        "st.sidebar.title(\"📖 How to use\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "1. Upload a traffic video (MP4, AVI, MOV, WEBM).\n",
        "2. Click **'🚨 Check Violations'** to analyze the video.\n",
        "3. Review detected violations and download logs/clips.\n",
        "\"\"\")\n",
        "st.sidebar.info(\"This tool uses AI to detect red-light running, jaywalking, and helmetless riding and other Infractions.\")\n",
        "\n",
        "# -------------------------------\n",
        "# Main Title\n",
        "# -------------------------------\n",
        "st.title(\"🚦 Smart Traffic Violation Detector\")\n",
        "st.markdown(\"\"\"\n",
        "Upload a traffic video to detect potential violations like **red-light running**,\n",
        "**jaywalking**, or **helmetless riding**.\n",
        "\"\"\")\n",
        "\n",
        "# -------------------------------\n",
        "# File Uploader\n",
        "# -------------------------------\n",
        "uploaded_video = st.file_uploader(\n",
        "    \"📤 Upload a video file to analyze\",\n",
        "    type=[\"mp4\", \"avi\", \"mov\", \"webm\"]\n",
        ")\n",
        "\n",
        "# Initialize session state\n",
        "if \"violations\" not in st.session_state:\n",
        "    st.session_state[\"violations\"] = None\n",
        "    st.session_state[\"log_ready\"] = False\n",
        "\n",
        "# -------------------------------\n",
        "# Main Logic\n",
        "# -------------------------------\n",
        "if uploaded_video:\n",
        "    st.video(uploaded_video)\n",
        "\n",
        "    # Save uploaded video to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video:\n",
        "        temp_video.write(uploaded_video.read())\n",
        "        temp_video_path = temp_video.name\n",
        "\n",
        "    if st.button(\"🚨 Check Violations\"):\n",
        "        st.info(\"⚙️ Starting analysis... please wait.\")\n",
        "        with st.spinner(\"Analyzing video for traffic violations...\"):\n",
        "            try:\n",
        "                processed_result, violations = main_func(temp_video_path)\n",
        "\n",
        "                # Save results to session state\n",
        "                st.session_state[\"violations\"] = violations\n",
        "                st.session_state[\"log_ready\"] = True\n",
        "\n",
        "                if violations:\n",
        "                    st.success(f\"✅ Analysis complete: {len(violations)} violation(s) detected.\")\n",
        "                else:\n",
        "                    st.success(\"✅ No violations detected in the video.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ An error occurred during analysis:\\n\\n{e}\")\n",
        "\n",
        "            finally:\n",
        "                # Clean up temporary video file\n",
        "                os.remove(temp_video_path)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Display Results (if any)\n",
        "    # -------------------------------\n",
        "    if st.session_state[\"log_ready\"]:\n",
        "        violations = st.session_state[\"violations\"]\n",
        "\n",
        "        # Download violation log\n",
        "        log_json = json.dumps(violations, indent=2)\n",
        "        st.download_button(\n",
        "            label=\"⬇️ Download Violation Log\",\n",
        "            data=log_json,\n",
        "            file_name=\"violation_log.json\",\n",
        "            mime=\"application/json\"\n",
        "        )\n",
        "\n",
        "        # log inline\n",
        "        if st.checkbox(\"📄 View Violation Log\"):\n",
        "            st.code(log_json, language=\"json\")\n",
        "\n",
        "        # Show detected violation clips\n",
        "        if violations:\n",
        "            st.subheader(\"📹 Violation Clips\")\n",
        "            for i, violation in enumerate(violations):\n",
        "                st.markdown(f\"---\\n### 🚨 Violation {i+1}: {violation['violation_type']}\")\n",
        "                st.write(f\"**Description**: {violation['description']}\")\n",
        "                st.write(f\"**Score**: {violation['score']:.2f}\")\n",
        "\n",
        "                clip_path = os.path.join(\n",
        "                    \"/content/smart_traffic_detector/clips\", violation[\"clip\"]\n",
        "                )\n",
        "\n",
        "                if os.path.exists(clip_path):\n",
        "                    st.video(clip_path)\n",
        "                    with open(clip_path, \"rb\") as clip_file:\n",
        "                        st.download_button(\n",
        "                            label=f\"⬇️ Download Clip {i+1}\",\n",
        "                            data=clip_file.read(),\n",
        "                            file_name=violation[\"clip\"],\n",
        "                            mime=\"video/mp4\",\n",
        "                            key=f\"download_clip_{i}\"\n",
        "                        )\n",
        "                else:\n",
        "                    st.warning(f\"⚠️ Clip not found: {violation['clip']}\")\n",
        "\n",
        "else:\n",
        "    st.info(\"👆 Upload a video file above to get started.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmGXDP9vDFmM"
      },
      "source": [
        "#### Write ` __init__file`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5mWrRgLm9ty"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/smart_traffic_detector/main/__init__.py\n",
        "\n",
        "from .main_app import main_func\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPxDEIh3bHkf"
      },
      "outputs": [],
      "source": [
        "!rm -rf __pycache__ main/__pycache__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ_CjDqvHY3R"
      },
      "source": [
        "#### 4. Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4GS0bnqSMPcI"
      },
      "outputs": [],
      "source": [
        "!pip install --q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux-dltNYmCPA"
      },
      "outputs": [],
      "source": [
        "!cp /content/mmaction2/tools/data/kinetics/label_map_k400.txt /content/smart_traffic_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5SURyYbvkeB"
      },
      "source": [
        "## `4. Local Testing`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zZemTpfCC43"
      },
      "source": [
        "#### 1. Download Youtube Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ALbeDhSEQUAu"
      },
      "outputs": [],
      "source": [
        "!yt-dlp \"https://youtu.be/lBRWEeuuMpE?si=AKzynUYzr0CcVQCq\" -o /content/smart_traffic_detector/test_video_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ZhsEBrCKG_"
      },
      "source": [
        "#### 2. Local Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uggZ-CMSFj2T"
      },
      "outputs": [],
      "source": [
        "\n",
        " !python /content/smart_traffic_detector/main/main_app.py  --video /content/smart_traffic_detector/test_video_2.webm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdVRb5_Q1aWF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST8Pom1X1alZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f07luw5mH3aO"
      },
      "source": [
        "#  ***Frontend Setup***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtAZFU5HGUHg"
      },
      "source": [
        "#### 1. Get Local tunnel Password"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDnWgmS1k2To",
        "outputId": "c41dbc14-dfc1-4d10-da0a-44d136113306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.16.215.240"
          ]
        }
      ],
      "source": [
        "!curl ifconfig.me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snqazxVdGhnQ"
      },
      "source": [
        "#### 2. Deploy Streamlit Prototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EIQioFGIsgD8",
        "outputId": "d585c7ba-7381-4ef3-94f8-1ff9cfbdd3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.215.240:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/smart_traffic_detector/streamlit_app.py & npx localtunnel --port 8501\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "M5TMspYIHY3F",
        "-vooaEa-rSl4",
        "InmJuwGAwGG8",
        "5IKyEHT5wgdf",
        "p5mGyQQ_HY3M",
        "LjxwwFHdHY3N",
        "M7jptMM1HY3O",
        "rmGXDP9vDFmM",
        "dQ_CjDqvHY3R",
        "-zZemTpfCC43",
        "Z-ZhsEBrCKG_"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}